D:\ProgramData\anaconda3\envs\homework_2_1\python.exe D:/homework/Course_Design_of_artificial_intelligence/third/ResNet_Seblock/fangzhang_ResNet_kaggle_dalao/PhanDai_ResNet_seblock_kaggle_dalao/pre_test/0528newV1.py
Using device: cuda
数据增强：True | 训练：True | 测试：True
------ 模型结构和参数量 ------
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 112, 112]           9,408
       BatchNorm2d-2         [-1, 64, 112, 112]             128
              ReLU-3         [-1, 64, 112, 112]               0
         MaxPool2d-4           [-1, 64, 56, 56]               0
            Conv2d-5           [-1, 64, 56, 56]          36,864
       BatchNorm2d-6           [-1, 64, 56, 56]             128
              ReLU-7           [-1, 64, 56, 56]               0
            Conv2d-8           [-1, 64, 56, 56]          36,864
       BatchNorm2d-9           [-1, 64, 56, 56]             128
AdaptiveAvgPool2d-10             [-1, 64, 1, 1]               0
           Linear-11                    [-1, 4]             256
             ReLU-12                    [-1, 4]               0
           Linear-13                   [-1, 64]             256
          Sigmoid-14                   [-1, 64]               0
          SEBlock-15           [-1, 64, 56, 56]               0
             ReLU-16           [-1, 64, 56, 56]               0
     BasicBlockSE-17           [-1, 64, 56, 56]               0
           Conv2d-18           [-1, 64, 56, 56]          36,864
      BatchNorm2d-19           [-1, 64, 56, 56]             128
             ReLU-20           [-1, 64, 56, 56]               0
           Conv2d-21           [-1, 64, 56, 56]          36,864
      BatchNorm2d-22           [-1, 64, 56, 56]             128
AdaptiveAvgPool2d-23             [-1, 64, 1, 1]               0
           Linear-24                    [-1, 4]             256
             ReLU-25                    [-1, 4]               0
           Linear-26                   [-1, 64]             256
          Sigmoid-27                   [-1, 64]               0
          SEBlock-28           [-1, 64, 56, 56]               0
             ReLU-29           [-1, 64, 56, 56]               0
     BasicBlockSE-30           [-1, 64, 56, 56]               0
           Conv2d-31          [-1, 128, 28, 28]          73,728
      BatchNorm2d-32          [-1, 128, 28, 28]             256
             ReLU-33          [-1, 128, 28, 28]               0
           Conv2d-34          [-1, 128, 28, 28]         147,456
      BatchNorm2d-35          [-1, 128, 28, 28]             256
AdaptiveAvgPool2d-36            [-1, 128, 1, 1]               0
           Linear-37                    [-1, 8]           1,024
             ReLU-38                    [-1, 8]               0
           Linear-39                  [-1, 128]           1,024
          Sigmoid-40                  [-1, 128]               0
          SEBlock-41          [-1, 128, 28, 28]               0
           Conv2d-42          [-1, 128, 28, 28]           8,192
      BatchNorm2d-43          [-1, 128, 28, 28]             256
             ReLU-44          [-1, 128, 28, 28]               0
     BasicBlockSE-45          [-1, 128, 28, 28]               0
           Conv2d-46          [-1, 128, 28, 28]         147,456
      BatchNorm2d-47          [-1, 128, 28, 28]             256
             ReLU-48          [-1, 128, 28, 28]               0
           Conv2d-49          [-1, 128, 28, 28]         147,456
      BatchNorm2d-50          [-1, 128, 28, 28]             256
AdaptiveAvgPool2d-51            [-1, 128, 1, 1]               0
           Linear-52                    [-1, 8]           1,024
             ReLU-53                    [-1, 8]               0
           Linear-54                  [-1, 128]           1,024
          Sigmoid-55                  [-1, 128]               0
          SEBlock-56          [-1, 128, 28, 28]               0
             ReLU-57          [-1, 128, 28, 28]               0
     BasicBlockSE-58          [-1, 128, 28, 28]               0
           Conv2d-59          [-1, 256, 14, 14]         294,912
      BatchNorm2d-60          [-1, 256, 14, 14]             512
             ReLU-61          [-1, 256, 14, 14]               0
           Conv2d-62          [-1, 256, 14, 14]         589,824
      BatchNorm2d-63          [-1, 256, 14, 14]             512
AdaptiveAvgPool2d-64            [-1, 256, 1, 1]               0
           Linear-65                   [-1, 16]           4,096
             ReLU-66                   [-1, 16]               0
           Linear-67                  [-1, 256]           4,096
          Sigmoid-68                  [-1, 256]               0
          SEBlock-69          [-1, 256, 14, 14]               0
           Conv2d-70          [-1, 256, 14, 14]          32,768
      BatchNorm2d-71          [-1, 256, 14, 14]             512
             ReLU-72          [-1, 256, 14, 14]               0
     BasicBlockSE-73          [-1, 256, 14, 14]               0
           Conv2d-74          [-1, 256, 14, 14]         589,824
      BatchNorm2d-75          [-1, 256, 14, 14]             512
             ReLU-76          [-1, 256, 14, 14]               0
           Conv2d-77          [-1, 256, 14, 14]         589,824
      BatchNorm2d-78          [-1, 256, 14, 14]             512
AdaptiveAvgPool2d-79            [-1, 256, 1, 1]               0
           Linear-80                   [-1, 16]           4,096
             ReLU-81                   [-1, 16]               0
           Linear-82                  [-1, 256]           4,096
          Sigmoid-83                  [-1, 256]               0
          SEBlock-84          [-1, 256, 14, 14]               0
             ReLU-85          [-1, 256, 14, 14]               0
     BasicBlockSE-86          [-1, 256, 14, 14]               0
           Conv2d-87            [-1, 512, 7, 7]       1,179,648
      BatchNorm2d-88            [-1, 512, 7, 7]           1,024
             ReLU-89            [-1, 512, 7, 7]               0
           Conv2d-90            [-1, 512, 7, 7]       2,359,296
      BatchNorm2d-91            [-1, 512, 7, 7]           1,024
AdaptiveAvgPool2d-92            [-1, 512, 1, 1]               0
           Linear-93                   [-1, 32]          16,384
             ReLU-94                   [-1, 32]               0
           Linear-95                  [-1, 512]          16,384
          Sigmoid-96                  [-1, 512]               0
          SEBlock-97            [-1, 512, 7, 7]               0
           Conv2d-98            [-1, 512, 7, 7]         131,072
      BatchNorm2d-99            [-1, 512, 7, 7]           1,024
            ReLU-100            [-1, 512, 7, 7]               0
    BasicBlockSE-101            [-1, 512, 7, 7]               0
          Conv2d-102            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-103            [-1, 512, 7, 7]           1,024
            ReLU-104            [-1, 512, 7, 7]               0
          Conv2d-105            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-106            [-1, 512, 7, 7]           1,024
AdaptiveAvgPool2d-107            [-1, 512, 1, 1]               0
          Linear-108                   [-1, 32]          16,384
            ReLU-109                   [-1, 32]               0
          Linear-110                  [-1, 512]          16,384
         Sigmoid-111                  [-1, 512]               0
         SEBlock-112            [-1, 512, 7, 7]               0
            ReLU-113            [-1, 512, 7, 7]               0
    BasicBlockSE-114            [-1, 512, 7, 7]               0
AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0
          Linear-116                  [-1, 512]         262,656
            ReLU-117                  [-1, 512]               0
         Dropout-118                  [-1, 512]               0
          Linear-119                  [-1, 120]          61,560
================================================================
Total params: 11,587,768
Trainable params: 11,587,768
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 68.59
Params size (MB): 44.20
Estimated Total Size (MB): 113.36
----------------------------------------------------------------
参数量: 11.59M
模型大小: 44.24 MB
FLOPs: 3.65 GFLOPs/图像

==== Epoch 1/80 ====
Train: 100%|██████████| 256/256 [02:40<00:00,  1.59it/s]
Val: 100%|██████████| 64/64 [00:28<00:00,  2.22it/s]
Train Loss: 4.3147 | Val Loss: 3.5321 | Val Acc: 0.2318 | Top-5: 0.5726 | F1: 0.1757

==== Epoch 2/80 ====
Train: 100%|██████████| 256/256 [01:51<00:00,  2.31it/s]
Val: 100%|██████████| 64/64 [00:15<00:00,  4.13it/s]
Train Loss: 3.4635 | Val Loss: 2.9056 | Val Acc: 0.3345 | Top-5: 0.7178 | F1: 0.2804

==== Epoch 3/80 ====
Train: 100%|██████████| 256/256 [01:50<00:00,  2.31it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.71it/s]
Train Loss: 3.0684 | Val Loss: 2.5748 | Val Acc: 0.4528 | Top-5: 0.8156 | F1: 0.4103
Train:   0%|          | 0/256 [00:00<?, ?it/s]
==== Epoch 4/80 ====
Train: 100%|██████████| 256/256 [01:46<00:00,  2.40it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.61it/s]
Train Loss: 2.8710 | Val Loss: 2.4345 | Val Acc: 0.4939 | Top-5: 0.8337 | F1: 0.4599

==== Epoch 5/80 ====
Train: 100%|██████████| 256/256 [01:45<00:00,  2.44it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.63it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 2.7081 | Val Loss: 2.4506 | Val Acc: 0.4905 | Top-5: 0.8235 | F1: 0.4701

==== Epoch 6/80 ====
Train: 100%|██████████| 256/256 [01:48<00:00,  2.37it/s]
Val: 100%|██████████| 64/64 [00:14<00:00,  4.39it/s]
Train Loss: 2.5753 | Val Loss: 2.3835 | Val Acc: 0.5037 | Top-5: 0.8377 | F1: 0.4736
Train:   0%|          | 0/256 [00:00<?, ?it/s]
==== Epoch 7/80 ====
Train: 100%|██████████| 256/256 [01:51<00:00,  2.30it/s]
Val: 100%|██████████| 64/64 [00:15<00:00,  4.24it/s]
Train Loss: 2.4650 | Val Loss: 2.3010 | Val Acc: 0.5276 | Top-5: 0.8548 | F1: 0.5077

==== Epoch 8/80 ====
Train: 100%|██████████| 256/256 [01:51<00:00,  2.29it/s]
Val: 100%|██████████| 64/64 [00:14<00:00,  4.29it/s]
Train Loss: 2.3825 | Val Loss: 2.2341 | Val Acc: 0.5555 | Top-5: 0.8748 | F1: 0.5407
Train:   0%|          | 0/256 [00:00<?, ?it/s]
==== Epoch 9/80 ====
Train: 100%|██████████| 256/256 [01:49<00:00,  2.34it/s]
Val: 100%|██████████| 64/64 [00:14<00:00,  4.45it/s]
Train Loss: 2.3169 | Val Loss: 2.1155 | Val Acc: 0.5990 | Top-5: 0.8861 | F1: 0.5850

==== Epoch 10/80 ====
Train: 100%|██████████| 256/256 [01:48<00:00,  2.37it/s]
Val: 100%|██████████| 64/64 [00:14<00:00,  4.50it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 2.2421 | Val Loss: 2.2363 | Val Acc: 0.5589 | Top-5: 0.8611 | F1: 0.5449

==== Epoch 11/80 ====
Train: 100%|██████████| 256/256 [01:51<00:00,  2.29it/s]
Val: 100%|██████████| 64/64 [00:14<00:00,  4.46it/s]
Train Loss: 2.1736 | Val Loss: 2.1357 | Val Acc: 0.5736 | Top-5: 0.8831 | F1: 0.5543

==== Epoch 12/80 ====
Train: 100%|██████████| 256/256 [01:46<00:00,  2.40it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.58it/s]
Train Loss: 2.1243 | Val Loss: 2.1454 | Val Acc: 0.5844 | Top-5: 0.8841 | F1: 0.5738

==== Epoch 13/80 ====
Train: 100%|██████████| 256/256 [01:48<00:00,  2.36it/s]
Val: 100%|██████████| 64/64 [00:14<00:00,  4.29it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 2.0634 | Val Loss: 2.1305 | Val Acc: 0.5932 | Top-5: 0.8856 | F1: 0.5853

==== Epoch 14/80 ====
Train: 100%|██████████| 256/256 [01:51<00:00,  2.29it/s]
Val: 100%|██████████| 64/64 [00:14<00:00,  4.45it/s]
Train Loss: 2.0285 | Val Loss: 2.0972 | Val Acc: 0.5946 | Top-5: 0.8870 | F1: 0.5825
Train:   0%|          | 0/256 [00:00<?, ?it/s]
==== Epoch 15/80 ====
Train: 100%|██████████| 256/256 [01:48<00:00,  2.36it/s]
Val: 100%|██████████| 64/64 [00:14<00:00,  4.34it/s]
Train Loss: 1.9662 | Val Loss: 2.0525 | Val Acc: 0.6098 | Top-5: 0.8900 | F1: 0.6000
Train:   0%|          | 0/256 [00:00<?, ?it/s]
==== Epoch 16/80 ====
Train: 100%|██████████| 256/256 [01:57<00:00,  2.18it/s]
Val: 100%|██████████| 64/64 [00:15<00:00,  4.14it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.9436 | Val Loss: 2.1672 | Val Acc: 0.5814 | Top-5: 0.8743 | F1: 0.5716

==== Epoch 17/80 ====
Train: 100%|██████████| 256/256 [01:51<00:00,  2.29it/s]
Val: 100%|██████████| 64/64 [00:14<00:00,  4.53it/s]
Train Loss: 1.8925 | Val Loss: 2.1115 | Val Acc: 0.5922 | Top-5: 0.8812 | F1: 0.5824

==== Epoch 18/80 ====
Train: 100%|██████████| 256/256 [01:54<00:00,  2.24it/s]
Val: 100%|██████████| 64/64 [00:14<00:00,  4.39it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.8422 | Val Loss: 2.1462 | Val Acc: 0.5897 | Top-5: 0.8743 | F1: 0.5807

==== Epoch 19/80 ====
Train: 100%|██████████| 256/256 [01:54<00:00,  2.24it/s]
Val: 100%|██████████| 64/64 [00:14<00:00,  4.55it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.8223 | Val Loss: 2.1844 | Val Acc: 0.5800 | Top-5: 0.8704 | F1: 0.5754

==== Epoch 20/80 ====
Train: 100%|██████████| 256/256 [02:02<00:00,  2.09it/s]
Val: 100%|██████████| 64/64 [00:14<00:00,  4.31it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.7803 | Val Loss: 2.1028 | Val Acc: 0.5995 | Top-5: 0.8738 | F1: 0.5884

==== Epoch 21/80 ====
Train: 100%|██████████| 256/256 [01:54<00:00,  2.24it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.60it/s]
Train Loss: 1.7659 | Val Loss: 2.1649 | Val Acc: 0.5878 | Top-5: 0.8768 | F1: 0.5749
Epoch 00021: reducing learning rate of group 0 to 1.5000e-04.

==== Epoch 22/80 ====
Train: 100%|██████████| 256/256 [01:48<00:00,  2.35it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.63it/s]
Train Loss: 1.5967 | Val Loss: 2.0227 | Val Acc: 0.6230 | Top-5: 0.8958 | F1: 0.6137

==== Epoch 23/80 ====
Train: 100%|██████████| 256/256 [01:43<00:00,  2.48it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.79it/s]
Train Loss: 1.5386 | Val Loss: 1.9903 | Val Acc: 0.6396 | Top-5: 0.8954 | F1: 0.6339

==== Epoch 24/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.52it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.77it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.5083 | Val Loss: 2.0159 | Val Acc: 0.6269 | Top-5: 0.8910 | F1: 0.6189

==== Epoch 25/80 ====
Train: 100%|██████████| 256/256 [01:42<00:00,  2.50it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.74it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.4818 | Val Loss: 2.0219 | Val Acc: 0.6264 | Top-5: 0.8846 | F1: 0.6184

==== Epoch 26/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.65it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.4853 | Val Loss: 1.9985 | Val Acc: 0.6303 | Top-5: 0.8905 | F1: 0.6239

==== Epoch 27/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.85it/s]
Train Loss: 1.4276 | Val Loss: 2.0003 | Val Acc: 0.6215 | Top-5: 0.8875 | F1: 0.6149

==== Epoch 28/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.79it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.4411 | Val Loss: 2.0515 | Val Acc: 0.6122 | Top-5: 0.8807 | F1: 0.6033

==== Epoch 29/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.78it/s]
Train Loss: 1.4072 | Val Loss: 2.0236 | Val Acc: 0.6230 | Top-5: 0.8822 | F1: 0.6157
Epoch 00029: reducing learning rate of group 0 to 7.5000e-05.

==== Epoch 30/80 ====
Train: 100%|██████████| 256/256 [01:42<00:00,  2.49it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.77it/s]
Train Loss: 1.3669 | Val Loss: 1.9774 | Val Acc: 0.6425 | Top-5: 0.8914 | F1: 0.6347
Train:   0%|          | 0/256 [00:00<?, ?it/s]
==== Epoch 31/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.73it/s]
Train Loss: 1.3294 | Val Loss: 1.9691 | Val Acc: 0.6342 | Top-5: 0.8846 | F1: 0.6269
Train:   0%|          | 0/256 [00:00<?, ?it/s]
==== Epoch 32/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.73it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.3081 | Val Loss: 1.9770 | Val Acc: 0.6357 | Top-5: 0.8910 | F1: 0.6322

==== Epoch 33/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.75it/s]
Train Loss: 1.2895 | Val Loss: 1.9674 | Val Acc: 0.6460 | Top-5: 0.8866 | F1: 0.6392

==== Epoch 34/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.75it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.2964 | Val Loss: 1.9726 | Val Acc: 0.6416 | Top-5: 0.8914 | F1: 0.6348

==== Epoch 35/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.70it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.2923 | Val Loss: 1.9740 | Val Acc: 0.6377 | Top-5: 0.8924 | F1: 0.6310

==== Epoch 36/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.79it/s]
Train Loss: 1.2824 | Val Loss: 1.9828 | Val Acc: 0.6401 | Top-5: 0.8831 | F1: 0.6353

==== Epoch 37/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.77it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.2596 | Val Loss: 1.9850 | Val Acc: 0.6352 | Top-5: 0.8875 | F1: 0.6295

==== Epoch 38/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.79it/s]
Train Loss: 1.2613 | Val Loss: 1.9701 | Val Acc: 0.6421 | Top-5: 0.8934 | F1: 0.6353

==== Epoch 39/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.79it/s]
Train Loss: 1.2467 | Val Loss: 1.9810 | Val Acc: 0.6430 | Top-5: 0.8929 | F1: 0.6373
Epoch 00039: reducing learning rate of group 0 to 3.7500e-05.

==== Epoch 40/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.79it/s]
Train Loss: 1.2244 | Val Loss: 1.9600 | Val Acc: 0.6450 | Top-5: 0.8939 | F1: 0.6397

==== Epoch 41/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.84it/s]
Train Loss: 1.2133 | Val Loss: 1.9570 | Val Acc: 0.6494 | Top-5: 0.8924 | F1: 0.6454
Train:   0%|          | 0/256 [00:00<?, ?it/s]
==== Epoch 42/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.81it/s]
Train Loss: 1.2008 | Val Loss: 1.9691 | Val Acc: 0.6474 | Top-5: 0.8885 | F1: 0.6396

==== Epoch 43/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.81it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.2073 | Val Loss: 1.9721 | Val Acc: 0.6430 | Top-5: 0.8905 | F1: 0.6370

==== Epoch 44/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.52it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.69it/s]
Train Loss: 1.2133 | Val Loss: 1.9868 | Val Acc: 0.6469 | Top-5: 0.8846 | F1: 0.6419

==== Epoch 45/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.82it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1975 | Val Loss: 1.9764 | Val Acc: 0.6401 | Top-5: 0.8861 | F1: 0.6335

==== Epoch 46/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.81it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.2050 | Val Loss: 1.9765 | Val Acc: 0.6411 | Top-5: 0.8866 | F1: 0.6378

==== Epoch 47/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.80it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1843 | Val Loss: 1.9896 | Val Acc: 0.6362 | Top-5: 0.8851 | F1: 0.6303
Epoch 00047: reducing learning rate of group 0 to 1.8750e-05.

==== Epoch 48/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.83it/s]
Train Loss: 1.1818 | Val Loss: 1.9765 | Val Acc: 0.6445 | Top-5: 0.8885 | F1: 0.6409

==== Epoch 49/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.75it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1710 | Val Loss: 1.9795 | Val Acc: 0.6425 | Top-5: 0.8851 | F1: 0.6373

==== Epoch 50/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.80it/s]
Train Loss: 1.1813 | Val Loss: 1.9868 | Val Acc: 0.6450 | Top-5: 0.8870 | F1: 0.6409

==== Epoch 51/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.81it/s]
Train Loss: 1.1684 | Val Loss: 1.9899 | Val Acc: 0.6430 | Top-5: 0.8846 | F1: 0.6362

==== Epoch 52/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.81it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1728 | Val Loss: 1.9620 | Val Acc: 0.6523 | Top-5: 0.8870 | F1: 0.6475

==== Epoch 53/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.79it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1565 | Val Loss: 1.9799 | Val Acc: 0.6474 | Top-5: 0.8851 | F1: 0.6418
Epoch 00053: reducing learning rate of group 0 to 9.3750e-06.

==== Epoch 54/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.80it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1719 | Val Loss: 1.9747 | Val Acc: 0.6499 | Top-5: 0.8846 | F1: 0.6455

==== Epoch 55/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.71it/s]
Train Loss: 1.1617 | Val Loss: 1.9725 | Val Acc: 0.6494 | Top-5: 0.8895 | F1: 0.6430

==== Epoch 56/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.72it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1600 | Val Loss: 1.9786 | Val Acc: 0.6460 | Top-5: 0.8861 | F1: 0.6398

==== Epoch 57/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.76it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1594 | Val Loss: 1.9783 | Val Acc: 0.6474 | Top-5: 0.8866 | F1: 0.6423

==== Epoch 58/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.75it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1541 | Val Loss: 1.9867 | Val Acc: 0.6455 | Top-5: 0.8870 | F1: 0.6400

==== Epoch 59/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.75it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1646 | Val Loss: 1.9867 | Val Acc: 0.6440 | Top-5: 0.8875 | F1: 0.6365
Epoch 00059: reducing learning rate of group 0 to 4.6875e-06.

==== Epoch 60/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.75it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1485 | Val Loss: 1.9878 | Val Acc: 0.6465 | Top-5: 0.8846 | F1: 0.6396

==== Epoch 61/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.84it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1543 | Val Loss: 1.9767 | Val Acc: 0.6479 | Top-5: 0.8875 | F1: 0.6398

==== Epoch 62/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.76it/s]
Train Loss: 1.1478 | Val Loss: 1.9781 | Val Acc: 0.6479 | Top-5: 0.8890 | F1: 0.6418

==== Epoch 63/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.69it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1535 | Val Loss: 1.9820 | Val Acc: 0.6435 | Top-5: 0.8895 | F1: 0.6368

==== Epoch 64/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.74it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1366 | Val Loss: 1.9750 | Val Acc: 0.6469 | Top-5: 0.8885 | F1: 0.6398

==== Epoch 65/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.83it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1469 | Val Loss: 1.9679 | Val Acc: 0.6499 | Top-5: 0.8900 | F1: 0.6464
Epoch 00065: reducing learning rate of group 0 to 2.3437e-06.

==== Epoch 66/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.78it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1478 | Val Loss: 1.9723 | Val Acc: 0.6509 | Top-5: 0.8885 | F1: 0.6426

==== Epoch 67/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.74it/s]
Train Loss: 1.1540 | Val Loss: 1.9778 | Val Acc: 0.6494 | Top-5: 0.8890 | F1: 0.6444

==== Epoch 68/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.73it/s]
Train Loss: 1.1469 | Val Loss: 1.9780 | Val Acc: 0.6479 | Top-5: 0.8841 | F1: 0.6429

==== Epoch 69/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.77it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1425 | Val Loss: 1.9696 | Val Acc: 0.6499 | Top-5: 0.8890 | F1: 0.6419

==== Epoch 70/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.84it/s]
Train Loss: 1.1367 | Val Loss: 1.9886 | Val Acc: 0.6440 | Top-5: 0.8846 | F1: 0.6374

==== Epoch 71/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.79it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1400 | Val Loss: 1.9686 | Val Acc: 0.6504 | Top-5: 0.8914 | F1: 0.6426
Epoch 00071: reducing learning rate of group 0 to 1.1719e-06.

==== Epoch 72/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.71it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1414 | Val Loss: 1.9725 | Val Acc: 0.6474 | Top-5: 0.8870 | F1: 0.6417

==== Epoch 73/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.52it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.78it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1522 | Val Loss: 1.9812 | Val Acc: 0.6494 | Top-5: 0.8875 | F1: 0.6441

==== Epoch 74/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.81it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1432 | Val Loss: 1.9789 | Val Acc: 0.6474 | Top-5: 0.8929 | F1: 0.6405

==== Epoch 75/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.82it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1456 | Val Loss: 1.9750 | Val Acc: 0.6494 | Top-5: 0.8885 | F1: 0.6422

==== Epoch 76/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.52it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.74it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1398 | Val Loss: 1.9844 | Val Acc: 0.6469 | Top-5: 0.8841 | F1: 0.6402

==== Epoch 77/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.54it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.80it/s]
Train Loss: 1.1564 | Val Loss: 1.9773 | Val Acc: 0.6455 | Top-5: 0.8841 | F1: 0.6391
Epoch 00077: reducing learning rate of group 0 to 5.8594e-07.

==== Epoch 78/80 ====
Train: 100%|██████████| 256/256 [01:40<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.81it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1356 | Val Loss: 1.9768 | Val Acc: 0.6499 | Top-5: 0.8861 | F1: 0.6431

==== Epoch 79/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.84it/s]
Train:   0%|          | 0/256 [00:00<?, ?it/s]Train Loss: 1.1433 | Val Loss: 1.9811 | Val Acc: 0.6465 | Top-5: 0.8841 | F1: 0.6409

==== Epoch 80/80 ====
Train: 100%|██████████| 256/256 [01:41<00:00,  2.53it/s]
Val: 100%|██████████| 64/64 [00:13<00:00,  4.86it/s]
Train Loss: 1.1427 | Val Loss: 1.9776 | Val Acc: 0.6489 | Top-5: 0.8875 | F1: 0.6423
收敛到最优Val Loss Epoch: 41，总训练用时: 157.93 分钟
SpeedTest: 100%|██████████| 64/64 [00:14<00:00,  4.54it/s]
推理平均测试速度: 2.440 ms/图像
推理最大显存占用: 425.24 MB
Kaggle Test: 100%|██████████| 324/324 [02:02<00:00,  2.66it/s]
Kaggle提交文件保存至: D:\homework\Course_Design_of_artificial_intelligence\third\ResNet_Seblock\fangzhang_ResNet_kaggle_dalao\PhanDai_ResNet_seblock_kaggle_dalao\pre_test\0528newV1\submission_se.csv

进程已结束,退出代码0
