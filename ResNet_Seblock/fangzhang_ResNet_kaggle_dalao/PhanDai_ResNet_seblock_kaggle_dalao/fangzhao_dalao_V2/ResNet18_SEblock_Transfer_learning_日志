D:\ProgramData\anaconda3\envs\homework_2_1\python.exe D:/homework/Course_Design_of_artificial_intelligence/third/ResNet_Seblock/fangzhang_ResNet_kaggle_dalao/PhanDai_ResNet_seblock_kaggle_dalao/fangzhao_dalao_V2.py

--- Model Summary ---
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 112, 112]           9,408
       BatchNorm2d-2         [-1, 64, 112, 112]             128
              ReLU-3         [-1, 64, 112, 112]               0
         MaxPool2d-4           [-1, 64, 56, 56]               0
            Conv2d-5           [-1, 64, 56, 56]          36,864
       BatchNorm2d-6           [-1, 64, 56, 56]             128
              ReLU-7           [-1, 64, 56, 56]               0
            Conv2d-8           [-1, 64, 56, 56]          36,864
       BatchNorm2d-9           [-1, 64, 56, 56]             128
AdaptiveAvgPool2d-10             [-1, 64, 1, 1]               0
           Linear-11                    [-1, 4]             256
             ReLU-12                    [-1, 4]               0
           Linear-13                   [-1, 64]             256
          Sigmoid-14                   [-1, 64]               0
          SEBlock-15           [-1, 64, 56, 56]               0
             ReLU-16           [-1, 64, 56, 56]               0
     SEBasicBlock-17           [-1, 64, 56, 56]               0
           Conv2d-18           [-1, 64, 56, 56]          36,864
      BatchNorm2d-19           [-1, 64, 56, 56]             128
             ReLU-20           [-1, 64, 56, 56]               0
           Conv2d-21           [-1, 64, 56, 56]          36,864
      BatchNorm2d-22           [-1, 64, 56, 56]             128
AdaptiveAvgPool2d-23             [-1, 64, 1, 1]               0
           Linear-24                    [-1, 4]             256
             ReLU-25                    [-1, 4]               0
           Linear-26                   [-1, 64]             256
          Sigmoid-27                   [-1, 64]               0
          SEBlock-28           [-1, 64, 56, 56]               0
             ReLU-29           [-1, 64, 56, 56]               0
     SEBasicBlock-30           [-1, 64, 56, 56]               0
           Conv2d-31          [-1, 128, 28, 28]          73,728
      BatchNorm2d-32          [-1, 128, 28, 28]             256
             ReLU-33          [-1, 128, 28, 28]               0
           Conv2d-34          [-1, 128, 28, 28]         147,456
      BatchNorm2d-35          [-1, 128, 28, 28]             256
AdaptiveAvgPool2d-36            [-1, 128, 1, 1]               0
           Linear-37                    [-1, 8]           1,024
             ReLU-38                    [-1, 8]               0
           Linear-39                  [-1, 128]           1,024
          Sigmoid-40                  [-1, 128]               0
          SEBlock-41          [-1, 128, 28, 28]               0
           Conv2d-42          [-1, 128, 28, 28]           8,192
      BatchNorm2d-43          [-1, 128, 28, 28]             256
             ReLU-44          [-1, 128, 28, 28]               0
     SEBasicBlock-45          [-1, 128, 28, 28]               0
           Conv2d-46          [-1, 128, 28, 28]         147,456
      BatchNorm2d-47          [-1, 128, 28, 28]             256
             ReLU-48          [-1, 128, 28, 28]               0
           Conv2d-49          [-1, 128, 28, 28]         147,456
      BatchNorm2d-50          [-1, 128, 28, 28]             256
AdaptiveAvgPool2d-51            [-1, 128, 1, 1]               0
           Linear-52                    [-1, 8]           1,024
             ReLU-53                    [-1, 8]               0
           Linear-54                  [-1, 128]           1,024
          Sigmoid-55                  [-1, 128]               0
          SEBlock-56          [-1, 128, 28, 28]               0
             ReLU-57          [-1, 128, 28, 28]               0
     SEBasicBlock-58          [-1, 128, 28, 28]               0
           Conv2d-59          [-1, 256, 14, 14]         294,912
      BatchNorm2d-60          [-1, 256, 14, 14]             512
             ReLU-61          [-1, 256, 14, 14]               0
           Conv2d-62          [-1, 256, 14, 14]         589,824
      BatchNorm2d-63          [-1, 256, 14, 14]             512
AdaptiveAvgPool2d-64            [-1, 256, 1, 1]               0
           Linear-65                   [-1, 16]           4,096
             ReLU-66                   [-1, 16]               0
           Linear-67                  [-1, 256]           4,096
          Sigmoid-68                  [-1, 256]               0
          SEBlock-69          [-1, 256, 14, 14]               0
           Conv2d-70          [-1, 256, 14, 14]          32,768
      BatchNorm2d-71          [-1, 256, 14, 14]             512
             ReLU-72          [-1, 256, 14, 14]               0
     SEBasicBlock-73          [-1, 256, 14, 14]               0
           Conv2d-74          [-1, 256, 14, 14]         589,824
      BatchNorm2d-75          [-1, 256, 14, 14]             512
             ReLU-76          [-1, 256, 14, 14]               0
           Conv2d-77          [-1, 256, 14, 14]         589,824
      BatchNorm2d-78          [-1, 256, 14, 14]             512
AdaptiveAvgPool2d-79            [-1, 256, 1, 1]               0
           Linear-80                   [-1, 16]           4,096
             ReLU-81                   [-1, 16]               0
           Linear-82                  [-1, 256]           4,096
          Sigmoid-83                  [-1, 256]               0
          SEBlock-84          [-1, 256, 14, 14]               0
             ReLU-85          [-1, 256, 14, 14]               0
     SEBasicBlock-86          [-1, 256, 14, 14]               0
           Conv2d-87            [-1, 512, 7, 7]       1,179,648
      BatchNorm2d-88            [-1, 512, 7, 7]           1,024
             ReLU-89            [-1, 512, 7, 7]               0
           Conv2d-90            [-1, 512, 7, 7]       2,359,296
      BatchNorm2d-91            [-1, 512, 7, 7]           1,024
AdaptiveAvgPool2d-92            [-1, 512, 1, 1]               0
           Linear-93                   [-1, 32]          16,384
             ReLU-94                   [-1, 32]               0
           Linear-95                  [-1, 512]          16,384
          Sigmoid-96                  [-1, 512]               0
          SEBlock-97            [-1, 512, 7, 7]               0
           Conv2d-98            [-1, 512, 7, 7]         131,072
      BatchNorm2d-99            [-1, 512, 7, 7]           1,024
            ReLU-100            [-1, 512, 7, 7]               0
    SEBasicBlock-101            [-1, 512, 7, 7]               0
          Conv2d-102            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-103            [-1, 512, 7, 7]           1,024
            ReLU-104            [-1, 512, 7, 7]               0
          Conv2d-105            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-106            [-1, 512, 7, 7]           1,024
AdaptiveAvgPool2d-107            [-1, 512, 1, 1]               0
          Linear-108                   [-1, 32]          16,384
            ReLU-109                   [-1, 32]               0
          Linear-110                  [-1, 512]          16,384
         Sigmoid-111                  [-1, 512]               0
         SEBlock-112            [-1, 512, 7, 7]               0
            ReLU-113            [-1, 512, 7, 7]               0
    SEBasicBlock-114            [-1, 512, 7, 7]               0
AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0
          Linear-116                  [-1, 120]          61,560
================================================================
Total params: 11,325,112
Trainable params: 11,325,112
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 68.57
Params size (MB): 43.20
Estimated Total Size (MB): 112.35
----------------------------------------------------------------
参数量: 11.33 M
模型大小: 43.24 MB
FLOPs: 3.65 GFLOPs/图像

Epoch 1/30 - train: 100%|██████████| 256/256 [02:04<00:00,  2.06it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.43it/s]
Epoch 1: train loss 3.8336, val loss 2.6197, val acc 0.4377, val top5 0.8098, val f1 0.3749
Epoch 2/30 - train: 100%|██████████| 256/256 [02:04<00:00,  2.06it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.31it/s]
Epoch 2: train loss 2.2581, val loss 1.7176, val acc 0.5844, val top5 0.8851, val f1 0.5582
Epoch 3/30 - train: 100%|██████████| 256/256 [02:02<00:00,  2.10it/s]
val: 100%|██████████| 64/64 [00:13<00:00,  4.61it/s]
Epoch 3: train loss 1.4309, val loss 1.3903, val acc 0.6220, val top5 0.9071, val f1 0.6078
Epoch 4/30 - train: 100%|██████████| 256/256 [01:57<00:00,  2.18it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.52it/s]
Epoch 4: train loss 1.0065, val loss 1.2195, val acc 0.6533, val top5 0.9144, val f1 0.6423
Epoch 5/30 - train: 100%|██████████| 256/256 [02:03<00:00,  2.08it/s]
val: 100%|██████████| 64/64 [00:15<00:00,  4.26it/s]
Epoch 5: train loss 0.7469, val loss 1.1907, val acc 0.6504, val top5 0.9193, val f1 0.6400
Epoch 6/30 - train: 100%|██████████| 256/256 [02:01<00:00,  2.10it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.33it/s]
Epoch 6: train loss 0.5715, val loss 1.2051, val acc 0.6504, val top5 0.9193, val f1 0.6453
Epoch 7/30 - train: 100%|██████████| 256/256 [01:59<00:00,  2.14it/s]
val: 100%|██████████| 64/64 [00:15<00:00,  4.01it/s]
Epoch 7: train loss 0.4518, val loss 1.1934, val acc 0.6479, val top5 0.9237, val f1 0.6403
Epoch 8/30 - train: 100%|██████████| 256/256 [02:02<00:00,  2.08it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.48it/s]
Epoch 8: train loss 0.3155, val loss 1.0982, val acc 0.6778, val top5 0.9291, val f1 0.6693
Epoch 9/30 - train: 100%|██████████| 256/256 [01:59<00:00,  2.15it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.34it/s]
Epoch 9: train loss 0.2671, val loss 1.0851, val acc 0.6841, val top5 0.9311, val f1 0.6762
Epoch 10/30 - train: 100%|██████████| 256/256 [01:56<00:00,  2.19it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.38it/s]
Epoch 10: train loss 0.2580, val loss 1.0793, val acc 0.6870, val top5 0.9306, val f1 0.6802
Epoch 11/30 - train: 100%|██████████| 256/256 [01:56<00:00,  2.20it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.42it/s]
Epoch 11: train loss 0.2386, val loss 1.0795, val acc 0.6822, val top5 0.9296, val f1 0.6756
Epoch 12/30 - train: 100%|██████████| 256/256 [01:58<00:00,  2.17it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.34it/s]
Epoch 12: train loss 0.2191, val loss 1.0816, val acc 0.6782, val top5 0.9276, val f1 0.6720
Epoch 13/30 - train: 100%|██████████| 256/256 [01:58<00:00,  2.16it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.56it/s]
Epoch 13: train loss 0.2216, val loss 1.0791, val acc 0.6836, val top5 0.9286, val f1 0.6765
Epoch 14/30 - train: 100%|██████████| 256/256 [02:01<00:00,  2.11it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.27it/s]
Epoch 14: train loss 0.1952, val loss 1.0788, val acc 0.6831, val top5 0.9247, val f1 0.6764
Epoch 15/30 - train: 100%|██████████| 256/256 [01:58<00:00,  2.16it/s]
val: 100%|██████████| 64/64 [00:13<00:00,  4.67it/s]
Epoch 15: train loss 0.1898, val loss 1.0799, val acc 0.6885, val top5 0.9306, val f1 0.6818
Epoch 16/30 - train: 100%|██████████| 256/256 [01:57<00:00,  2.18it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.45it/s]
Epoch 16: train loss 0.1860, val loss 1.0765, val acc 0.6905, val top5 0.9276, val f1 0.6831
Epoch 17/30 - train: 100%|██████████| 256/256 [02:04<00:00,  2.06it/s]
val: 100%|██████████| 64/64 [00:15<00:00,  4.09it/s]
Epoch 17: train loss 0.1848, val loss 1.0808, val acc 0.6836, val top5 0.9291, val f1 0.6763
Epoch 18/30 - train: 100%|██████████| 256/256 [01:55<00:00,  2.22it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.55it/s]
Epoch 18: train loss 0.1886, val loss 1.0798, val acc 0.6885, val top5 0.9286, val f1 0.6815
Epoch 19/30 - train: 100%|██████████| 256/256 [01:55<00:00,  2.21it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.44it/s]
Epoch 19: train loss 0.1924, val loss 1.0781, val acc 0.6929, val top5 0.9271, val f1 0.6872
Epoch 20/30 - train: 100%|██████████| 256/256 [01:56<00:00,  2.20it/s]
val: 100%|██████████| 64/64 [00:13<00:00,  4.63it/s]
Epoch 20: train loss 0.1881, val loss 1.0849, val acc 0.6890, val top5 0.9262, val f1 0.6825
Epoch 21/30 - train: 100%|██████████| 256/256 [01:52<00:00,  2.27it/s]
val: 100%|██████████| 64/64 [00:13<00:00,  4.62it/s]
Epoch 21: train loss 0.1902, val loss 1.0820, val acc 0.6861, val top5 0.9271, val f1 0.6795
Epoch 22/30 - train: 100%|██████████| 256/256 [01:53<00:00,  2.25it/s]
val: 100%|██████████| 64/64 [00:13<00:00,  4.61it/s]
Epoch 22: train loss 0.1789, val loss 1.0795, val acc 0.6866, val top5 0.9301, val f1 0.6808
Epoch 23/30 - train: 100%|██████████| 256/256 [01:53<00:00,  2.25it/s]
val: 100%|██████████| 64/64 [00:13<00:00,  4.60it/s]
Epoch 23: train loss 0.1872, val loss 1.0822, val acc 0.6856, val top5 0.9267, val f1 0.6795
Epoch 24/30 - train: 100%|██████████| 256/256 [01:54<00:00,  2.24it/s]
val: 100%|██████████| 64/64 [00:13<00:00,  4.62it/s]
Epoch 24: train loss 0.1772, val loss 1.0787, val acc 0.6870, val top5 0.9301, val f1 0.6808
Epoch 25/30 - train: 100%|██████████| 256/256 [01:58<00:00,  2.15it/s]
val: 100%|██████████| 64/64 [00:15<00:00,  4.23it/s]
Epoch 25: train loss 0.1743, val loss 1.0767, val acc 0.6870, val top5 0.9296, val f1 0.6817
Epoch 26/30 - train: 100%|██████████| 256/256 [01:57<00:00,  2.18it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.49it/s]
Epoch 26: train loss 0.1726, val loss 1.0756, val acc 0.6900, val top5 0.9311, val f1 0.6848
Epoch 27/30 - train: 100%|██████████| 256/256 [01:56<00:00,  2.20it/s]
val: 100%|██████████| 64/64 [00:13<00:00,  4.58it/s]
Epoch 27: train loss 0.1816, val loss 1.0781, val acc 0.6826, val top5 0.9281, val f1 0.6768
Epoch 28/30 - train: 100%|██████████| 256/256 [01:57<00:00,  2.19it/s]
val: 100%|██████████| 64/64 [00:14<00:00,  4.51it/s]
Epoch 28: train loss 0.1853, val loss 1.0788, val acc 0.6870, val top5 0.9281, val f1 0.6822
Epoch 29/30 - train: 100%|██████████| 256/256 [01:56<00:00,  2.20it/s]
val: 100%|██████████| 64/64 [00:15<00:00,  4.22it/s]
Epoch 29: train loss 0.1788, val loss 1.0820, val acc 0.6831, val top5 0.9271, val f1 0.6781
Epoch 30/30 - train: 100%|██████████| 256/256 [02:01<00:00,  2.11it/s]
val: 100%|██████████| 64/64 [00:13<00:00,  4.63it/s]
Epoch 30: train loss 0.1724, val loss 1.0774, val acc 0.6875, val top5 0.9291, val f1 0.6803

收敛到最优Val Loss Epoch: 26，总训练用时: 66.52 分钟

======= Final Validation Set Evaluation =======
Eval: 100%|██████████| 64/64 [00:16<00:00,  3.98it/s]
SpeedTest:   0%|          | 0/64 [00:00<?, ?it/s]Validation Top-1 Accuracy: 0.6875
Validation Top-5 Accuracy: 0.9291
SpeedTest: 100%|██████████| 64/64 [00:14<00:00,  4.47it/s]
推理平均测试速度: 2.269 ms/图像
推理最大显存占用: 437.91 MB
predict: 100%|██████████| 324/324 [01:20<00:00,  4.02it/s]
提交文件已保存: D:\homework\Course_Design_of_artificial_intelligence\third\ResNet_Seblock\fangzhang_ResNet_kaggle_dalao\PhanDai_ResNet_seblock_kaggle_dalao\fangzhao_dalao_V2\submission.csv

进程已结束,退出代码0
